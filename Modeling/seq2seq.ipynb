{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 3000 #가장 많이 사용된 3000개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__() #Model(Encoder)?\n",
    "        self.emb = Embedding(NUM_WORDS, 64)\n",
    "        \n",
    "        self.lstm = LSTM(512, return_state=True) #return_State : hidden, cell state 둘다 출력해주기 위한 옵션\n",
    "        \n",
    "    def __call__(self, x, training=False, mask=None):\n",
    "        x = self.emb(x)\n",
    "        _, h, c = self.lstm(x) #출력벡터인 hidden/cell state를 받음. we discard encoder outputs.\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = Embedding(NUM_WORDS, 64)\n",
    "        \n",
    "        self.lstm = LSTM(512, return_sequences=True, return_state=True) #h1,...hn / c1,...cn까지 모두 출력받음\n",
    "        self.dense = Dense(NUM_WORDS, activation='softmax') #to make output\n",
    "        \n",
    "    def __call__(self, inputs, training=False, mask=None):\n",
    "        #x for each inputs\n",
    "        #h, c for inital state of h and c\n",
    "        x, h, c = inputs\n",
    "        x = self.emb(x) #x for what?\n",
    "        \n",
    "        outputs, _, _ = self.lstm(x, initial_state=[h,c]) \n",
    "        #x로 input 집어넣는다. 위로 나오는게 x(길이512). 옆으로 나오는게 h, c. discard h,c\n",
    "        return self.dense(outputs) #각 output x 에 dense를 취해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(Model):\n",
    "    def __init__(self, sos, eos):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        \n",
    "    def __call__(self, inputs, training=False, mask=None):\n",
    "        if training: #y 값이 input으로 있음\n",
    "            x, inp = inputs #x for encoder inputs. inp for decoder inputs\n",
    "            \n",
    "            h,c = self.enc(x)\n",
    "            \n",
    "            y = self.dec((inp,h,c)) #y는 softmax로 나타낸 모든 단어에 대한 확률값 (해당 위치에서의)\n",
    "            \n",
    "            return y #한꺼번에 y값 처리 가능\n",
    "        \n",
    "        else: #y값이 input으로 없음\n",
    "            '''shifted output for inference'''\n",
    "            x = inputs\n",
    "            h,c = self.enc(x)\n",
    "            \n",
    "            #decoder단에 가장 먼저 sos삽입\n",
    "            inp = tf.convert_to_tensor(self.sos)\n",
    "            inp = tf.reshape(inp, (1,1)) #input길이가 하나밖에 없음\n",
    "            \n",
    "            seq = tf.TensorArray(tf.int32, 64) #정답array - 최대 64길이까지 출력으로 받는다(summarization을)\n",
    "            \n",
    "            #shifted output\n",
    "            for _ in tf.range(64): #tf.range better for keras Models\n",
    "                y = self.dec([inp,h,c]) #input길이 하나밖에 없기때문에 한개값 출력받을 것임\n",
    "                \n",
    "                #shifted output\n",
    "                inp = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32) #for next input\n",
    "                inp = tf.reshape(inp, (1,1))\n",
    "                \n",
    "                seq = seq.write(_,inp)\n",
    "                \n",
    "                if inp == self.eos:\n",
    "                    break\n",
    "            \n",
    "            return tf.reshape(seq.stack(), (1,64)) # stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다.   \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, inputs, labels, loss_object, opt, train_loss, train_acc):\n",
    "    output_labels = labels[:,1:] #real labels - to compare with lstm outputs - 제일첫번째는 sos\n",
    "    shifted_labels = labels[:,:-1] #shifted_labels & what goes for the decoder inputs - 제일마지막은 eos\n",
    "    \n",
    "    Input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
