{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Embedding, LSTM, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 3000 #본문 기사 내 가장 많이 사용된 3000단어\n",
    "VECTOR_SIZE_NOT_DETERMINED_YET = 100\n",
    "MAX_SUMMARIZATION_LENGTH_NOT_DETERMINED_YET = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self):\n",
    "        self.VECTOR_SIZE_NOT_DETERMINED_YET = 100\n",
    "        self.MAX_SUMMARIZATION_LENGTH_NOT_DETERMINED_YET = 500\n",
    "        \n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        \n",
    "        self.optimizer = Adam()\n",
    "        \n",
    "        encoder_inputs = Input(shape=(self.VECTOR_SIZE_NOT_DETERMINED_YET,))\n",
    "        decoder_inputs = Input(shape=(self.MAX_SUMMARIZATION_LENGTH_NOT_DETERMINED_YET,))\n",
    "        \n",
    "        self.enc = self.build_encoder()\n",
    "        hc_list = self.enc(encoder_inputs)\n",
    "        \n",
    "        self.dec = self.build_decoder(hc_list)\n",
    "        summary_softmaxed = self.dec(decoder_inputs)\n",
    "        \n",
    "        #ㅁ//summary_softmaxed는 전체 3000개 freq 단어(3000)에 대한 softmax값..?으로해야하는건지\n",
    "        #ㅁ//Or 데이터 내 전체 단어에 대한 softmax값으로 써야할 지 모르겠음\n",
    "        \n",
    "        self.seq2seq = Model(inputs=encoder_inputs, outputs=summary_softmaxed)\n",
    "        self.seq2seq.compile(loss='cross_categorical_entropy', optimizer=self.optimizer)\n",
    "        \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        def lstm(inputs, hs):\n",
    "            output,h,c = LSTM(hs, return_state=True)(inputs) #return only last h, c\n",
    "#             e = Dropout(0.3)(e)\n",
    "            return output, h, c\n",
    "        \n",
    "        #(encoder input) already embedded from koBERT   \n",
    "        encoder_inputs = Input(shape=(self.VECTOR_SIZE_NOT_DETERMINED_YET,))\n",
    "        _, h, c = lstm(encoder_inputs, 512) #Discard encoder outputs\n",
    "        \n",
    "        return Model(encoder_inputs, [h,c])\n",
    "    \n",
    "    \n",
    "    def build_decoder(self, hc_list):\n",
    "        def lstm(inputs, hs, initial_state):\n",
    "            output,h,c = LSTM(hs, return_sequences=True, return_state=True)(inputs, initial_state=initial_state) #returns all -> h1,...hn / c1,...cn\n",
    "#             e = Dropout(0.3)(e)\n",
    "            return output, h, c\n",
    "\n",
    "        def fc(n_h_layers, inputs, hn):\n",
    "            for _ in range(n_h_layers):\n",
    "                d = Dense(hn, activation='tanh')(inputs)\n",
    "            if n_h_layers==0: \n",
    "                d = inputs\n",
    "            output = Dense(hn, activation='softmax')(d)\n",
    "            #모든 3000개 단어에 대한 확률값 (해당 위치에서의)\n",
    "            \n",
    "        decoder_inputs = Input(shape=(self.MAX_SUMMARIZATION_LENGTH_NOT_DETERMINED_YET,))\n",
    "        \n",
    "        outputs, _, _ = lstm(decoder_inputs, 512, hc_list) #Discard h,c\n",
    "        \n",
    "        outputs_softmaxed = fc(1, outputs, NUM_WORDS) #ㅁ//NUM_WORDS가 맞는지?\n",
    "        return Model(decoder_inputs, outputs_softmaxed)      \n",
    "            \n",
    "    def train(self, epochs, batch_size=1):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__() \n",
    "        self.lstm = LSTM(512, return_state=True) \n",
    "        \n",
    "    def __call__(self, x, training=False, mask=None):\n",
    "        \n",
    "        _, h, c = self.lstm(x) #Discard encoder outputs.\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = LSTM(512, return_sequences=True, return_state=True) #h1,...hn / c1,...cn까지 모두 출력받음\n",
    "        \n",
    "        self.dense1 = Dense(NUM_WORDS, activation='tanh')\n",
    "        self.dense2 = Dense(NUM_WORDS, activation='softmax') #to make output\n",
    "        #모든 3000개 단어에 대한 확률값 (해당 위치에서의)\n",
    "        \n",
    "    def __call__(self, inputs, training=False, mask=None):\n",
    "        #inputs given as (decoderinput, h, c)\n",
    "        #h, c for inital state of decoder lstm\n",
    "        #x(input) already embedded from koBERT\n",
    "        x, h, c = inputs\n",
    "        \n",
    "        outputs, _, _ = self.lstm(x, initial_state=[h,c]) #Discard h,c\n",
    "        \n",
    "        outputs_1 = self.dense1(outputs)\n",
    "        outputs_2 = self.dense2(outputs_1) \n",
    "        return outputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(Model):\n",
    "    def __init__(self, sos, eos):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        \n",
    "    def __call__(self, inputs, training=False):\n",
    "        if training:\n",
    "            x, inp = tuple(inputs) #x for encoder inputs. inp for decoder inputs\n",
    "            \n",
    "            h,c = self.enc(x)\n",
    "            \n",
    "            y = self.dec((inp,h,c)) \n",
    "            \n",
    "            return y\n",
    "        \n",
    "        else: #no inputs for decoder\n",
    "            '''shifted output for inference'''\n",
    "            x = inputs\n",
    "            h,c = self.enc(x)\n",
    "            \n",
    "            #decoder단에 가장 먼저 sos삽입\n",
    "            inp = tf.convert_to_tensor(self.sos)\n",
    "            inp = tf.reshape(inp, (1,1)) #input길이가 하나밖에 없음\n",
    "            \n",
    "            seq = tf.TensorArray(tf.int32, 64) #정답array - 최대 64길이까지 출력으로 받는다(summarization을)\n",
    "            \n",
    "            #shifted output\n",
    "            for _ in tf.range(64): #tf.range better for keras Models\n",
    "                y = self.dec([inp,h,c]) #input길이 하나밖에 없기때문에 한개값 출력받을 것임\n",
    "                \n",
    "                #shifted output\n",
    "                inp = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32) #for next input\n",
    "                inp = tf.reshape(inp, (1,1))\n",
    "                \n",
    "                seq = seq.write(_,inp)\n",
    "                \n",
    "                if inp == self.eos:\n",
    "                    break\n",
    "            \n",
    "            return tf.reshape(seq.stack(), (1,64)) # stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다.   \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
