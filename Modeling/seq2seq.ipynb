{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Embedding, LSTM, Dropout, Input, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300 #for padding\n",
    "NUM_WORDS = 3000 #본문 기사 내 가장 많이 사용된 3000단어?(or 전체 단어수로 할지 미정)\n",
    "VECTOR_SIZE_NOT_DETERMINED_YET = 100\n",
    "MAX_SUMMARIZATION_LENGTH_NOT_DETERMINED_YET = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self):\n",
    "        self.max_length = 300\n",
    "        self.VECTOR_SIZE_NOT_DETERMINED_YET = 100\n",
    "        \n",
    "#         self.sos =  sos #give some number\n",
    "#         self.eos =  eos #give some number\n",
    "        \n",
    "        self.optimizer = Adam()\n",
    "        \n",
    "        encoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        decoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        \n",
    "        self.seq2seq = self.build_seq2seq()\n",
    "\n",
    "        self.seq2seq.compile(loss='categorical_crossentropy', optimizer=self.optimizer)\n",
    "        print(self.seq2seq.summary())\n",
    "        \n",
    "        \n",
    "    def build_seq2seq(self):\n",
    "        def lstm(inputs, hs, seq=False, initial=None):\n",
    "            output,h,c = LSTM(hs, return_state=True, return_sequences=seq)(inputs, initial_state=initial) #return only last h, c\n",
    "            return output, h, c\n",
    "        \n",
    "        def fc(n_h_layers, inputs, hn):\n",
    "            for _ in range(n_h_layers):\n",
    "                d = Dense(hn, activation='tanh')(inputs)\n",
    "            if n_h_layers==0: \n",
    "                d = inputs\n",
    "            output = Dense(hn, activation='softmax')(d)\n",
    "            #모든 3000개 단어에 대한 확률값 (해당 위치에서의)\n",
    "            return output\n",
    "        \n",
    "        #(encoder input) already embedded from koBERT(vector size 만큼)\n",
    "        encoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        _, h, c = lstm(encoder_inputs, 256)  #Discard encoder outputs\n",
    "        init_states = [h,c]\n",
    "        \n",
    "        decoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        decoder_outputs, _, _ = lstm(decoder_inputs, 256, seq=True, initial=init_states) #Discard encoder outputs\n",
    "        \n",
    "        outputs_softmaxed = fc(1, decoder_outputs, NUM_WORDS) #ㅁ//NUM_WORDS가 맞는지?\n",
    "\n",
    "        mod = Model([encoder_inputs, decoder_inputs], outputs_softmaxed) \n",
    "        return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_137\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_214 (InputLayer)          (None, 300, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_215 (InputLayer)          (None, 300, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_104 (LSTM)                 [(None, 256), (None, 365568      input_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_105 (LSTM)                 [(None, 300, 256), ( 365568      input_215[0][0]                  \n",
      "                                                                 lstm_104[0][1]                   \n",
      "                                                                 lstm_104[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 300, 3000)    771000      lstm_105[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 300, 3000)    9003000     dense_40[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,505,136\n",
      "Trainable params: 10,505,136\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.seq2seq.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_168:0\", shape=(None, 300, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, RepeatVector, Dropout\n",
    "encoder_inputs = Input(shape=(300,100))\n",
    "print(encoder_inputs)\n",
    "encoder = LSTM(256, return_state=True) #only last h and c\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs) #h8(return sequence=false니깐), h8, c8\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_169:0\", shape=(None, 300, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#프랑스어(그앞전단어)\n",
    "decoder_inputs = Input(shape=(300,100))\n",
    "print(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states) ###디코더의h,c 초기값을 encoder껄로함 - 이걸로 encoder까지불러옴. enc,dec연결\n",
    "#h1부터h17까지(그리고h2는 첫번째단어만보고맞춘거,h3는 처음두개보고맞춘거...)\n",
    "decoder_outputs = Dense(2000,activation='tanh')(decoder_outputs)\n",
    "decoder_dense = Dense(3000, activation='softmax')(decoder_outputs)#다중입력,다중출력\n",
    "#+1한거는 unk이 있어서.\n",
    "\n",
    "model2 = Model([encoder_inputs, decoder_inputs], decoder_dense) \n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__() \n",
    "        self.lstm = LSTM(512, return_state=True) \n",
    "        \n",
    "    def __call__(self, x, training=False, mask=None):\n",
    "        \n",
    "        _, h, c = self.lstm(x) #Discard encoder outputs.\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = LSTM(512, return_sequences=True, return_state=True) #h1,...hn / c1,...cn까지 모두 출력받음\n",
    "        \n",
    "        self.dense1 = Dense(NUM_WORDS, activation='tanh')\n",
    "        self.dense2 = Dense(NUM_WORDS, activation='softmax') #to make output\n",
    "        #모든 3000개 단어에 대한 확률값 (해당 위치에서의)\n",
    "        \n",
    "    def __call__(self, inputs, training=False, mask=None):\n",
    "        #inputs given as (decoderinput, h, c)\n",
    "        #h, c for inital state of decoder lstm\n",
    "        #x(input) already embedded from koBERT\n",
    "        x, h, c = inputs\n",
    "        \n",
    "        outputs, _, _ = self.lstm(x, initial_state=[h,c]) #Discard h,c\n",
    "        \n",
    "        outputs_1 = self.dense1(outputs)\n",
    "        outputs_2 = self.dense2(outputs_1) \n",
    "        return outputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(Model):\n",
    "    def __init__(self, sos, eos):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        \n",
    "    def __call__(self, inputs, training=False):\n",
    "        if training:\n",
    "            x, inp = tuple(inputs) #x for encoder inputs. inp for decoder inputs\n",
    "            \n",
    "            h,c = self.enc(x)\n",
    "            \n",
    "            y = self.dec((inp,h,c)) \n",
    "            \n",
    "            return y\n",
    "        \n",
    "        else: #no inputs for decoder\n",
    "            '''shifted output for inference'''\n",
    "            x = inputs\n",
    "            h,c = self.enc(x)\n",
    "            \n",
    "            #decoder단에 가장 먼저 sos삽입\n",
    "            inp = tf.convert_to_tensor(self.sos)\n",
    "            inp = tf.reshape(inp, (1,1)) #input길이가 하나밖에 없음\n",
    "            \n",
    "            seq = tf.TensorArray(tf.int32, 64) #정답array - 최대 64길이까지 출력으로 받는다(summarization을)\n",
    "            \n",
    "            #shifted output\n",
    "            for _ in tf.range(64): #tf.range better for keras Models\n",
    "                y = self.dec([inp,h,c]) #input길이 하나밖에 없기때문에 한개값 출력받을 것임\n",
    "                \n",
    "                #shifted output\n",
    "                inp = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32) #for next input\n",
    "                inp = tf.reshape(inp, (1,1))\n",
    "                \n",
    "                seq = seq.write(_,inp)\n",
    "                \n",
    "                if inp == self.eos:\n",
    "                    break\n",
    "            \n",
    "            return tf.reshape(seq.stack(), (1,64)) # stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다.   \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
