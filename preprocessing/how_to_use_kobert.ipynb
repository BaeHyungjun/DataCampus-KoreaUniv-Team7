{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KOBERT\n",
    "\n",
    "처음엔 kobert가 단순히 문맥을 반영한 embedding을 해주는 것인줄 알았는데\n",
    "\n",
    "찾다보니까 해결하고 싶은 task별로 활용 방법이 다르더라고요.\n",
    "\n",
    "단순히 embedding으로 사용하는 task가 있고, transfer learning처럼 뒤에 prediction layer만 추가해서 사용하는 경우도 있더라고요.\n",
    "\n",
    "text summarization의 경우에는 아직 어떤 방식으로 사용하는게 정답인지는 모르겠는데\n",
    "\n",
    "지혜님이 모델 예쁘게 짜오셨으니까 그거로 학습할 수 있으면 하고 아니면 아래의 github 참고해서 하면 좋을 것 같아요. 뭔가 저희 문제에 맞는 모델인 것 같은데 tensorflow 코드를 빠르게 못 읽겠어서 일단 링크부터 공유합니다!\n",
    "\n",
    "https://github.com/raufer/bert-summarization\n",
    "\n",
    "https://github.com/kururuken/BERT-Transformer-for-Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설치\n",
    "\n",
    "pip install torch\n",
    "\n",
    "pip install mxnet\n",
    "\n",
    "pip install gluonnlp==0.8.0\n",
    "\n",
    "0.9.1은 이유를 모르겠는데 설치가 안되더라고요\n",
    "\n",
    "pip install sentencepiece\n",
    "\n",
    "pip install onnxruntime\n",
    "\n",
    "pip install transformers\n",
    "\n",
    "pip install kobert-transformers\n",
    "\n",
    "### kobert-transformers 사이트\n",
    "\n",
    "https://pypi.org/project/kobert-transformers/\n",
    "\n",
    "https://github.com/monologg/KoBERT-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:45.252757Z",
     "start_time": "2020-08-12T00:46:12.912496Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\golds\\AppData\\Roaming\\Python\\Python37\\site-packages\\mxnet\\optimizer\\optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
      "  Optimizer.opt_registry[name].__name__))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import mxnet\n",
    "import gluonnlp\n",
    "import sentencepiece\n",
    "import onnxruntime\n",
    "import transformers\n",
    "import kobert_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:45.270747Z",
     "start_time": "2020-08-12T00:47:45.256753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch :  1.6.0\n",
      "mxnet :  1.6.0\n",
      "gluonnlp :  0.8.0\n",
      "onnxruntime :  1.4.0\n",
      "transformers :  3.0.2\n",
      "kobert_transformers :  0.4.1\n"
     ]
    }
   ],
   "source": [
    "print('torch : ', torch.__version__)\n",
    "print('mxnet : ', mxnet.__version__)\n",
    "print('gluonnlp : ', gluonnlp.__version__)\n",
    "# print('sentencepiece : ', sentencepiece.__version__)\n",
    "print('onnxruntime : ', onnxruntime.__version__) \n",
    "print('transformers : ', transformers.__version__)\n",
    "print('kobert_transformers : ', kobert_transformers.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:45.426693Z",
     "start_time": "2020-08-12T00:47:45.276740Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예시로 사용할 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:45.854656Z",
     "start_time": "2020-08-12T00:47:45.429688Z"
    }
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('./news_cleaned.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:45.987249Z",
     "start_time": "2020-08-12T00:47:45.857648Z"
    }
   },
   "outputs": [],
   "source": [
    "query = cur.execute('SELECT * FROM daumnews LIMIT 0, 1000')\n",
    "cols = [column[0] for column in query.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:46.792971Z",
     "start_time": "2020-08-12T00:47:45.989236Z"
    }
   },
   "outputs": [],
   "source": [
    "daum = pd.DataFrame.from_records(data=query.fetchall(), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:46.800960Z",
     "start_time": "2020-08-12T00:47:46.794965Z"
    }
   },
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:46.916530Z",
     "start_time": "2020-08-12T00:47:46.805958Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"벌금과 단속 걱정하던 아들, 장사 준비하던 현수막으로 목 메 숨져\"결혼까지 미루고 식당 영업재개할 생각에 청소까지 열심히 했는데···\"\n",
      "\n",
      " \"아버지 아들인게 정말 행복했습니다. 이 가게가 잘 될 수 있도록 수호신이 될게요\"\n",
      "26살 황승우씨는 지난 30일 오전 경기 남양주시 조안면 막국수집 주방에서 목을 메 숨진 채 발견됐다. 이곳은 아버지 황선남(65)씨와 지난 2015년 희망을 품고 문을 연 가게였다.\n",
      "건물주의 운전기사로 시작해 세를 얻어 희망을 품고 시작한 막국수 집은 인상이 서글서글한 부자의 웃음과 열의로 그나마 장사가 되는 듯 했다.\n",
      "그러나 이곳은 팔당상수원 환경정비구역으로 남양주시와 의정부지검이 지난해 부터 단속을 벌여 음식점을 운영하던 7명이 구속됐다.\n",
      "황씨가 운영하던 막국수 집도 단속을 피할 수 없었다. 지난해 12월 단속을 더이상 견딜 수 없었던 이들은 결국 음식점 문을 닫았다.\n",
      "문제는 수입은 없는데 검찰이 내린 벌금 3000만원과 남양주시의 이행강제금 3690만원을 감당하기에는 너무나 힘에 부쳤다. 주변 가게에서 아르바이트를 해도 생활비는 커녕 벌금과 과태료도 낼 수 없었다.\n",
      "결국 승우씨는 주변 지인과 카드빚을 내 휴가철 관광객들을 대상으로 소시지와 커피 등을 판매하는 노점상을 하기로 했다. 교제하던 여자친구와는 결혼도 미뤘다.\n",
      "하지만 이마저도 한강유역환경청이 휴가철 수질을 오염시킬 우려가 있다며 지난달 집중단속을 하면서 승우씨의 노점상도 손해만 본 채 접게 됐다. 승우씨는 지인들 뿐 아니라 금융권에서도 신용불량자의 낙인이 찍힌 채 절망에 빠졌다.\n",
      "이런 가운데 환경청에서 일부 원주민들에 대해서는 음식점 운영을 할 수 있도록 제도를 개선한다는 소식이 전해지면서 한줄기 희망이 생기는 듯 했다. 승우씨는 다시 장사를 할 수도 있다는 생각에 문을 닫았던 음식점의 주방부터 청소를 시작했다.\n",
      "하지만 환경청이 주민들의 의견을 묵살하면서 이같은 희망도 오래가지 못했다. 승우씨는 청소를 위해 음식점을 찾았다가 이같은 소식을 접했다.\n",
      "\n",
      "음식점 앞에는 독촉장이 쌓여 있었고 결국 승우씨는 자신이 청소해 오던 주방에서 노점상에 걸어 둔 현수막에 목을 메 스스로 목숨을 끊었다.\n",
      "승우씨는 목을 메는 순간에도 수사에 대한 두려움을 견디지 못한 자신을 탓하며 아버지를 걱정했다. A4용지 2장짜리 분량에는 \"수사도 두려울 뿐더러 잘 산 것 하나 없는 아들이라 죄송합니다. 늘 건강하시고 행복을 잃지 마세요\"라고 썼다.\n",
      "지난달 31일 오후 취재진이 찾은 승우씨의 장례시장 곳곳에서는 울음소리가 끊이지 않았다.\n",
      "승우씨의 아버지는 \"사람이 살 수 있게 몰아 부쳐야지 정부가 갈 곳 없이 벼랑 끝까지 아들을 내몰아 끝내 죽음을 선택하게 했다\"며 흐느꼈다.\n",
      "그는 \"빚까지 져가며 살아보겠다고 노점상 하나 차렸는데 이마저도 단속 당한다는 두려움과 다시 장사를 할 수 있다는 희망마저 끊어진 상태에서 이곳저곳 독촉까지 아들이 견뎌야 하는 짐이 너무 컸다\"며 \"아버지의 수호신이 된다는 아들의 마지막 말은 견딜 수 없을 정도로 아프다\"고 더이상 말을 잇지 못했다.\n",
      "승우씨의 지인은 \"불법이라는 것은 인정하지만 그렇다면 정상적으로 신고를 하고 영업할 수 있도록 해야지 무조건 규제를 하고 이를 빌미로 단속을 하는 것을 반복하는 것은 원주민들을 죽이기 위한 반복된 악행\"이라며 \"살 수 있는 방법은 조금이라도 마련해 줘야 하는 것 아니냐\"고 한탄했다.\n",
      "한편 조안면 주민들은 1일 오전 상여집회를 열기로 했다.\n"
     ]
    }
   ],
   "source": [
    "print(daum.body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:47.037034Z",
     "start_time": "2020-08-12T00:47:46.920529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26살 황승우씨는 지난 30일 오전 경기 남양주시 조안면 막국수집 주방에서 목을 메 숨진 채 발견됐다.\n",
      "이곳은 아버지 황선남(65)씨와 지난 2015년 희망을 품고 문을 연 가게였다.\n",
      "황씨가 운영하던 막국수 집도 단속을 피할 수 없었다.\n",
      "하지만 이마저도 한강유역환경청이 휴가철 수질을 오염시킬 우려가 있다며 지난달 집중단속을 하면서 승우씨의 노점상도 손해만 본 채 접게 됐다.\n"
     ]
    }
   ],
   "source": [
    "print(daum.summary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sktbrain-kobert 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:47.353308Z",
     "start_time": "2020-08-12T00:47:47.040031Z"
    }
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:47.886401Z",
     "start_time": "2020-08-12T00:47:47.357307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tok_path = get_tokenizer()\n",
    "sp  = SentencepieceTokenizer(tok_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:47.911385Z",
     "start_time": "2020-08-12T00:47:47.888401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁\"',\n",
       " '벌',\n",
       " '금',\n",
       " '과',\n",
       " '▁단속',\n",
       " '▁걱정',\n",
       " '하던',\n",
       " '▁아들',\n",
       " ',',\n",
       " '▁장',\n",
       " '사',\n",
       " '▁준비',\n",
       " '하던',\n",
       " '▁현',\n",
       " '수',\n",
       " '막',\n",
       " '으로',\n",
       " '▁목',\n",
       " '▁메',\n",
       " '▁숨']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_body_token = sp(daum.body[0])\n",
    "example_body_token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T03:25:04.831221Z",
     "start_time": "2020-08-12T03:25:04.778254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁26',\n",
       " '살',\n",
       " '▁황',\n",
       " '승',\n",
       " '우',\n",
       " '씨는',\n",
       " '▁지난',\n",
       " '▁30',\n",
       " '일',\n",
       " '▁오전',\n",
       " '▁경기',\n",
       " '▁남',\n",
       " '양',\n",
       " '주',\n",
       " '시',\n",
       " '▁조',\n",
       " '안',\n",
       " '면',\n",
       " '▁막',\n",
       " '국',\n",
       " '수',\n",
       " '집',\n",
       " '▁주',\n",
       " '방',\n",
       " '에서',\n",
       " '▁목',\n",
       " '을',\n",
       " '▁메',\n",
       " '▁숨진',\n",
       " '▁채',\n",
       " '▁발견됐다',\n",
       " '.',\n",
       " '▁이곳',\n",
       " '은',\n",
       " '▁아버지',\n",
       " '▁황',\n",
       " '선',\n",
       " '남',\n",
       " '(',\n",
       " '65',\n",
       " ')',\n",
       " '씨와',\n",
       " '▁지난',\n",
       " '▁2015',\n",
       " '년',\n",
       " '▁희망',\n",
       " '을',\n",
       " '▁품',\n",
       " '고',\n",
       " '▁문',\n",
       " '을',\n",
       " '▁연',\n",
       " '▁',\n",
       " '가',\n",
       " '게',\n",
       " '였다',\n",
       " '.',\n",
       " '▁황',\n",
       " '씨가',\n",
       " '▁운영',\n",
       " '하던',\n",
       " '▁막',\n",
       " '국',\n",
       " '수',\n",
       " '▁집',\n",
       " '도',\n",
       " '▁단속',\n",
       " '을',\n",
       " '▁피',\n",
       " '할',\n",
       " '▁수',\n",
       " '▁없었다',\n",
       " '.',\n",
       " '▁하지만',\n",
       " '▁이',\n",
       " '마저',\n",
       " '도',\n",
       " '▁한강',\n",
       " '유',\n",
       " '역',\n",
       " '환경',\n",
       " '청',\n",
       " '이',\n",
       " '▁휴가',\n",
       " '철',\n",
       " '▁수',\n",
       " '질',\n",
       " '을',\n",
       " '▁오',\n",
       " '염',\n",
       " '시킬',\n",
       " '▁우려가',\n",
       " '▁있다',\n",
       " '며',\n",
       " '▁지난달',\n",
       " '▁집중',\n",
       " '단',\n",
       " '속',\n",
       " '을',\n",
       " '▁하면서',\n",
       " '▁승',\n",
       " '우',\n",
       " '씨의',\n",
       " '▁노',\n",
       " '점',\n",
       " '상',\n",
       " '도',\n",
       " '▁손',\n",
       " '해',\n",
       " '만',\n",
       " '▁본',\n",
       " '▁채',\n",
       " '▁접',\n",
       " '게',\n",
       " '▁됐다',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_summary_token = sp(daum.summary[0])\n",
    "example_summary_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떻게 단어를 id로 바꾸는지 모르겠음...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kobert_transformers 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:48.167436Z",
     "start_time": "2020-08-12T00:47:48.041500Z"
    }
   },
   "outputs": [],
   "source": [
    "from kobert_transformers import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:50.677709Z",
     "start_time": "2020-08-12T00:47:48.170434Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:50.715669Z",
     "start_time": "2020-08-12T00:47:50.679690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁\"',\n",
       " '벌',\n",
       " '금',\n",
       " '과',\n",
       " '▁단속',\n",
       " '▁걱정',\n",
       " '하던',\n",
       " '▁아들',\n",
       " ',',\n",
       " '▁장',\n",
       " '사',\n",
       " '▁준비',\n",
       " '하던',\n",
       " '▁현',\n",
       " '수',\n",
       " '막',\n",
       " '으로',\n",
       " '▁목',\n",
       " '▁메',\n",
       " '▁숨']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_body_token = tokenizer.tokenize(daum.body[0])\n",
    "example_body_token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:50.832896Z",
     "start_time": "2020-08-12T00:47:50.718670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁26',\n",
       " '살',\n",
       " '▁황',\n",
       " '승',\n",
       " '우',\n",
       " '씨는',\n",
       " '▁지난',\n",
       " '▁30',\n",
       " '일',\n",
       " '▁오전',\n",
       " '▁경기',\n",
       " '▁남',\n",
       " '양',\n",
       " '주',\n",
       " '시',\n",
       " '▁조',\n",
       " '안',\n",
       " '면',\n",
       " '▁막',\n",
       " '국']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_summary_token = tokenizer.tokenize(daum.summary[0])\n",
    "example_summary_token[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:50.944991Z",
     "start_time": "2020-08-12T00:47:50.836893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[518,\n",
       " 6332,\n",
       " 5550,\n",
       " 5468,\n",
       " 1590,\n",
       " 880,\n",
       " 7802,\n",
       " 3107,\n",
       " 46,\n",
       " 3954,\n",
       " 6493,\n",
       " 4249,\n",
       " 7802,\n",
       " 5049,\n",
       " 6629,\n",
       " 6149,\n",
       " 7078,\n",
       " 2068,\n",
       " 2016,\n",
       " 2919]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_body_token_id = tokenizer.convert_tokens_to_ids(example_body_token)\n",
    "example_body_token_id[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:51.087502Z",
     "start_time": "2020-08-12T00:47:50.948989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[584,\n",
       " 6519,\n",
       " 5149,\n",
       " 6703,\n",
       " 7005,\n",
       " 6787,\n",
       " 4304,\n",
       " 592,\n",
       " 7126,\n",
       " 3431,\n",
       " 956,\n",
       " 1409,\n",
       " 6853,\n",
       " 7276,\n",
       " 6705,\n",
       " 4162,\n",
       " 6812,\n",
       " 6198,\n",
       " 1927,\n",
       " 5503]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_summary_token_id = tokenizer.convert_tokens_to_ids(example_summary_token)\n",
    "example_summary_token_id[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sktbrain-kobert 예시 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T21:09:59.319343Z",
     "start_time": "2020-08-09T21:09:59.291360Z"
    }
   },
   "source": [
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T21:10:06.201707Z",
     "start_time": "2020-08-09T21:10:00.919977Z"
    }
   },
   "source": [
    "model, vocab  = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kobert_transformers 예시 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:47:51.206118Z",
     "start_time": "2020-08-12T00:47:51.090499Z"
    }
   },
   "outputs": [],
   "source": [
    "from kobert_transformers import get_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:48:48.416298Z",
     "start_time": "2020-08-12T00:48:43.168460Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_kobert_model()\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:49:08.109679Z",
     "start_time": "2020-08-12T00:49:08.104684Z"
    }
   },
   "outputs": [],
   "source": [
    "example_body_tensor = torch.LongTensor([example_body_token_id])\n",
    "example_summary_tensor = torch.LongTensor([example_summary_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:49:08.686200Z",
     "start_time": "2020-08-12T00:49:08.680203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 883])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_body_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:49:09.507506Z",
     "start_time": "2020-08-12T00:49:09.496514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index out of range in self\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    body_sequence_output, body_pooled_output = model(example_body_tensor)\n",
    "except IndexError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding vector의 차원이 768이니까 input의 길이가 768보다 커서 index error가 뜨는것인가...?\n",
    "\n",
    "단어 개수가 768개보다 작은거만 input으로 넣을 수 있다는게 말이 안되지 않나...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:49:10.996046Z",
     "start_time": "2020-08-12T00:49:10.456382Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_sequence_output, summary_pooled_output = model(example_summary_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:49:11.188927Z",
     "start_time": "2020-08-12T00:49:11.179932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 116])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_summary_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T00:49:11.389802Z",
     "start_time": "2020-08-12T00:49:11.382807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 116, 768]), torch.Size([1, 768]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_sequence_output.shape, summary_pooled_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1개의 sample\n",
    "\n",
    "단어수가 116개\n",
    "\n",
    "embedding vector의 차원이 768\n",
    "\n",
    "문장의 시작에 [CLS], 문장의 끝에 [SEP]를 무조건 넣어줘야하나??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT has a constraint on the maximum length of a sequence after tokenizing. For any BERT model, the maximum sequence length after tokenization is 512. But we can set any sequence length equal to or below this value. For faster training, I’ll be using 128 as the maximum sequence length. A bigger number may give better results if there are sequences longer than this value.\n",
    "\n",
    "토큰화를 한 후 sequence의 최대 길이는 512\n",
    "\n",
    "글의 시작에 [CLS]를 적어주니까 사용할 수 있는 토큰 개수는 511"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding만 해주는게 아니고\n",
    "\n",
    "embedding + pre-trained modeling이라서 저희가 짜야하는 모델은 그 뒤에 붙혀서 학습할 모델이 되야하는건지??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
