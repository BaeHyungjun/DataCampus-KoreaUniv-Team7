{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "body_kobert_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIk-0D0sz7uT",
        "colab_type": "text"
      },
      "source": [
        "코랩쓰고나서 암이 나았습니다.\n",
        "\n",
        "여기서는 램 12기가까지 지원하는데 얼마나 썼는지 계속 확인할 수 있어서 확인해보니\n",
        "\n",
        "이제까지 몇십시간씩 걸린건 역시 램이 딸려서 작동을 멈춰버린 상태여서 그런거였네요\n",
        "\n",
        "여기서도 20만개 한번에 돌리니까 메모리 에러 나서 5만개씩 쪼개서 4번 돌려야겠어요\n",
        "\n",
        "메모리 문제때문에 pickle로 내렸다가 올렸다가 반복하면서 했습니다\n",
        "\n",
        "요약하면 ast.literal_eval => convert token to id => padding만 했고 이제 kobert로 embedding 단계에요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cygojTloaowD",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pip install kobert-transformers\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IECWgsLpbhPb",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "from kobert_transformers import get_tokenizer\n",
        "from kobert_transformers import get_kobert_model\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezoPaP1PpaKe",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# 구글 드라이브 마운트 시키기\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXFKfNeOt5Q-",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "cd /content/mnt/My Drive/Colab Notebooks\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KezvIo2mpCpP",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "data = pd.read_csv('news_tokenized.csv')\n",
        "data.info()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFQJPHuj3IIC",
        "colab_type": "text"
      },
      "source": [
        "# body_morphs에 ast.literal_eval 시켜주기 => token 파일\n",
        "\n",
        "body_morphs_list_0.pkl\n",
        "\n",
        "body_morphs_list_1.pkl\n",
        "\n",
        "body_morphs_list_2.pkl\n",
        "\n",
        "body_morphs_list_3.pkl\n",
        "\n",
        "파일 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlvkgvGBwxwc",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_0 = []\n",
        "\n",
        "start = time.time()\n",
        "for i in range(len(data.body_morphs[:50000])):\n",
        "    body_morphs_list_0.append(ast.literal_eval(data.body_morphs[i]))\n",
        "    now = time.time()\n",
        "    if i % 10000 == 0:\n",
        "      print('{}번째 작업, 걸린 시간 : {}분 {}초'.format(i,\n",
        "                                        round((now - start) // 60),\n",
        "                                        round((now - start) % 60)))\n",
        "      \n",
        "with open('body_morphs_list_0.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_0, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qvl944b2UXG",
        "colab_type": "text"
      },
      "source": [
        "위에 cell 돌아가는데 2분 걸림"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ65qk8zxL-v",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_1 = []\n",
        "\n",
        "for element in data.body_morphs[50000:100000]:\n",
        "    body_morphs_list_1.append(ast.literal_eval(element))\n",
        "\n",
        "with open('body_morphs_list_1.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_1, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcrIGPo81X2e",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_2 = []\n",
        "\n",
        "for element in data.body_morphs[100000:150000]:\n",
        "    body_morphs_list_2.append(ast.literal_eval(element))\n",
        "\n",
        "with open('body_morphs_list_2.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_2, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxpim7Sl1Yv-",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_3 = []\n",
        "\n",
        "for element in data.body_morphs[150000:]:\n",
        "    body_morphs_list_3.append(ast.literal_eval(element))\n",
        "\n",
        "with open('body_morphs_list_3.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_3, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrWhQ89y6YfG",
        "colab_type": "text"
      },
      "source": [
        "# 다시 읽어들여서 token을 token id로 변환\n",
        "\n",
        "bert의 maximum sequence length가 512이므로 token id를 512번째까지만 저장\n",
        "\n",
        "512 미만으로 token length를 가지고 있는 데이터는 뒤에서 zero padding을 해줄 예정\n",
        "\n",
        "Hannanum으로 tokenizing을 해서 kobert_tokenizer가 인식 못하는 token은 token_id가 0으로 변환되었음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj95DP9d68qX",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "tokenizer = get_tokenizer()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVrF9sa5OHV",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "with open('body_morphs_list_0.pkl', 'rb') as file:\n",
        "    body_morphs_list_0 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_0 = []\n",
        "\n",
        "for element in body_morphs_list_0:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_0.append(element_id[:512])\n",
        "\n",
        "with open('body_morphs_list_1.pkl', 'rb') as file:\n",
        "    body_morphs_list_1 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_1 = []\n",
        "\n",
        "for element in body_morphs_list_1:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_1.append(element_id[:512])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBtK2PZj9d-e",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_token_id_01 = body_morphs_token_id_0 + body_morphs_token_id_1\n",
        "\n",
        "with open('body_morphs_token_id_01.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_token_id_01, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fGrMUrG8xmy",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "with open('body_morphs_list_2.pkl', 'rb') as file:\n",
        "    body_morphs_list_2 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_2 = []\n",
        "\n",
        "for element in body_morphs_list_2:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_2.append(element_id[:512])\n",
        "\n",
        "with open('body_morphs_list_3.pkl', 'rb') as file:\n",
        "    body_morphs_list_3 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_3 = []\n",
        "\n",
        "for element in body_morphs_list_3:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_3.append(element_id[:512])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2vIu61S80gH",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_token_id_23 = body_morphs_token_id_2 + body_morphs_token_id_3\n",
        "\n",
        "with open('body_morphs_token_id_23.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_token_id_23, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8miUVs0_WVh",
        "colab_type": "text"
      },
      "source": [
        "# token_id 불러와서 kobert_embedding 작업"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k24uXiyzAE0B",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "with open('body_morphs_token_id_01.pkl', 'rb') as file:\n",
        "  body_morphs_token_id_01 = pickle.load(file)\n",
        "\n",
        "with open('body_morphs_token_id_23.pkl', 'rb') as file:\n",
        "  body_morphs_token_id_23 = pickle.load(file)\n",
        "\n",
        "body_token_id = body_morphs_token_id_01 + body_morphs_token_id_23\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMMQbRNPCGyJ",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "pad = tokenizer.pad_token_id\n",
        "\n",
        "print(pad) # 출력값 : 1\n",
        "\n",
        "dir(tokenizer)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYjem3XgKkl8",
        "colab_type": "text"
      },
      "source": [
        "# zero padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWDTyGv8BUKc",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# 길이가 512 미만인 애들 뒤쪽에 padding\n",
        "# 0은 tokenizer의 convert_token_to_id가 모르는 단어, 1은 패딩\n",
        "\n",
        "body_token_id_padding = []\n",
        "\n",
        "for element in body_token_id:\n",
        "  if len(element) < 512:\n",
        "    pad_element = element + [pad] * (512 - len(element))\n",
        "  else:\n",
        "    pad_element = element\n",
        "\n",
        "  body_token_id_padding.append(pad_element)\n",
        "\n",
        "with open('body_token_id_padding.pkl', 'wb') as file:\n",
        "  pickle.dump(body_token_id_padding, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZumKn_fiPtDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "63daef0e-95df-489e-995a-989bc8cf7758"
      },
      "source": [
        "pip install kobert-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kobert-transformers in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from kobert-transformers) (1.6.0+cu101)\n",
            "Requirement already satisfied: transformers>=2.9.1 in /usr/local/lib/python3.6/dist-packages (from kobert-transformers) (3.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->kobert-transformers) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->kobert-transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.9.1->kobert-transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.9.1->kobert-transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.9.1->kobert-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.9.1->kobert-transformers) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.9.1->kobert-transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isi0DBitPtwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "from kobert_transformers import get_tokenizer\n",
        "from kobert_transformers import get_kobert_model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNz9ARLlPw40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6c5ffc2a-9462-4723-db70-a45b4a716518"
      },
      "source": [
        "cd /content/mnt/My Drive/Colab Notebooks"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mnt/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGf36Q2eHW7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('body_token_id_padding.pkl', 'rb') as file:\n",
        "  body_token_id_padding = pickle.load(file)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqQhJsIXA5as",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = get_kobert_model()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba-Zzs7bKXdx",
        "colab_type": "text"
      },
      "source": [
        "# embedding 단계\n",
        "\n",
        "200000개를 해야되는데 10개씩밖에 변환 못하는거 실화인가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4yRP6B6BgiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "body_sequence_output_0, body_pooled_output_0 = model(torch.LongTensor(body_token_id_padding[:10]))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atO-evvaBrH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c296b63a-4954-44ff-e0fe-4768eae4764c"
      },
      "source": [
        "body_sequence_output_0.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 512, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0E6rC0oT6vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}