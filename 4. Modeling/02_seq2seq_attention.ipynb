{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Embedding, LSTM, Dropout, Input, Dense, dot, concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300 #for padding\n",
    "NUM_WORDS = 3000 #본문 기사 내 가장 많이 사용된 3000단어?(or 전체 단어수로 할지 미정)\n",
    "VECTOR_SIZE_NOT_DETERMINED_YET = 100\n",
    "MAX_SUMMARIZATION_LENGTH_NOT_DETERMINED_YET = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self):\n",
    "        self.max_length = 300\n",
    "        self.VECTOR_SIZE_NOT_DETERMINED_YET = 100\n",
    "        \n",
    "#         self.sos =  sos #give some number\n",
    "#         self.eos =  eos #give some number\n",
    "        \n",
    "        self.optimizer = Adam()\n",
    "        \n",
    "        encoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        decoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        \n",
    "        self.seq2seq = self.build_seq2seq()\n",
    "\n",
    "        self.seq2seq.compile(loss='categorical_crossentropy', optimizer=self.optimizer)\n",
    "        print(self.seq2seq.summary())\n",
    "        \n",
    "        \n",
    "    def build_seq2seq(self):\n",
    "        def lstm(inputs, hs, seq=False, initial=None):\n",
    "            output,h,c = LSTM(hs, return_state=True, return_sequences=seq)(inputs, initial_state=initial) #return only last h, c\n",
    "            return output, h, c\n",
    "        \n",
    "        def fc(n_h_layers, inputs, hn):\n",
    "            for _ in range(n_h_layers):\n",
    "                d = Dense(hn, activation='tanh')(inputs)\n",
    "            if n_h_layers==0: \n",
    "                d = inputs\n",
    "            output = Dense(hn, activation='softmax')(d)\n",
    "            #모든 3000개 단어에 대한 확률값 (해당 위치에서의)\n",
    "            return output\n",
    "        \n",
    "        #(encoder input) already embedded from koBERT(vector size 만큼)\n",
    "        encoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        _, h, c = lstm(encoder_inputs, 256)  #Discard encoder outputs\n",
    "        init_states = [h,c]\n",
    "        \n",
    "        decoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        decoder_outputs, _, _ = lstm(decoder_inputs, 256, seq=True, initial=init_states) #Discard encoder outputs\n",
    "        \n",
    "        outputs_softmaxed = fc(1, decoder_outputs, NUM_WORDS) #ㅁ//NUM_WORDS가 맞는지?\n",
    "\n",
    "        mod = Model([encoder_inputs, decoder_inputs], outputs_softmaxed) \n",
    "        return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_137\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_214 (InputLayer)          (None, 300, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_215 (InputLayer)          (None, 300, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_104 (LSTM)                 [(None, 256), (None, 365568      input_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_105 (LSTM)                 [(None, 300, 256), ( 365568      input_215[0][0]                  \n",
      "                                                                 lstm_104[0][1]                   \n",
      "                                                                 lstm_104[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 300, 3000)    771000      lstm_105[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 300, 3000)    9003000     dense_40[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,505,136\n",
      "Trainable params: 10,505,136\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.seq2seq.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Attention():\n",
    "    def __init__(self):\n",
    "        self.max_length = 300\n",
    "        self.VECTOR_SIZE_NOT_DETERMINED_YET = 100\n",
    "        \n",
    "#         self.sos =  sos #give some number\n",
    "#         self.eos =  eos #give some number\n",
    "        \n",
    "        self.optimizer = Adam()\n",
    "        \n",
    "        encoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        decoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        \n",
    "        self.att = self.build_att()\n",
    "\n",
    "        self.att.compile(loss='categorical_crossentropy', optimizer=self.optimizer)\n",
    "        print(self.att.summary())\n",
    "        \n",
    "        \n",
    "    def build_att(self):\n",
    "        def lstm(inputs, hs, seq=True, initial=None):\n",
    "            output,h,c = LSTM(hs, return_state=True, return_sequences=seq)(inputs, initial_state=initial) \n",
    "            #return_seq=False & return_state=True: return only last h, c\n",
    "            #return_seq=Ture & return_state=True: return all h, c\n",
    "            return output, h, c\n",
    "        \n",
    "        def fc(n_h_layers, inputs, hn):\n",
    "            for _ in range(n_h_layers):\n",
    "                d = Dense(hn, activation='tanh')(inputs)\n",
    "            if n_h_layers==0: \n",
    "                d = inputs\n",
    "            output = Dense(hn, activation='softmax')(d)\n",
    "            #모든 3000개 단어에 대한 확률값 (해당 위치에서의)\n",
    "            return output\n",
    "        \n",
    "        #(encoder input) already embedded from koBERT(vector size 만큼)\n",
    "        encoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        encoder_outputs, h, c = lstm(encoder_inputs, 256)  #Discard encoder outputs\n",
    "        print(encoder_outputs)\n",
    "        init_states = [h,c]\n",
    "        \n",
    "        decoder_inputs = Input(shape=(self.max_length,self.VECTOR_SIZE_NOT_DETERMINED_YET))\n",
    "        decoder_outputs, _, _ = lstm(decoder_inputs, 256, initial=init_states) #Discard encoder outputs\n",
    "        \n",
    "        value = Dense(5000, activation='tanh')(encoder_outputs)\n",
    "        query = Dense(5000, activation='tanh')(decoder_outputs)\n",
    "        print(value, query) #300x5000 두개\n",
    "        \n",
    "        attention = dot([query, value],axes=[2,2])\n",
    "        print(attention) #300x300\n",
    "        \n",
    "        attention_softmaxed = fc(0, attention, max_length)\n",
    "        print(attention_softmaxed) #300x300\n",
    "        print(encoder_outputs)\n",
    "        \n",
    "        weighted = dot([attention_softmaxed, encoder_outputs], axes=[2,1]) #give weights to encoder outputs(=각 토큰)\n",
    "        print(attention_softmaxed)\n",
    "        print(weighted)\n",
    "        \n",
    "        decoder_for_final = concatenate([weighted, decoder_outputs]) #weighted token(3000x256) + decoder output(3000x256)\n",
    "        #or add? 둘중에 성능좋은거 고르기\n",
    "        \n",
    "        decoder_final = fc(1, decoder_for_final, NUM_WORDS)\n",
    "        print(decoder_final)\n",
    "        \n",
    "        mod = Model([encoder_inputs, decoder_inputs], decoder_final) \n",
    "        return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm_7/transpose_1:0\", shape=(None, 300, 256), dtype=float32)\n",
      "Tensor(\"dense_10/Tanh:0\", shape=(None, 300, 5000), dtype=float32) Tensor(\"dense_11/Tanh:0\", shape=(None, 300, 5000), dtype=float32)\n",
      "Tensor(\"dot_7/MatMul:0\", shape=(None, 300, 300), dtype=float32)\n",
      "Tensor(\"dense_12/truediv:0\", shape=(None, 300, 300), dtype=float32)\n",
      "Tensor(\"lstm_7/transpose_1:0\", shape=(None, 300, 256), dtype=float32)\n",
      "Tensor(\"dense_12/truediv:0\", shape=(None, 300, 300), dtype=float32)\n",
      "Tensor(\"dot_8/MatMul:0\", shape=(None, 300, 256), dtype=float32)\n",
      "Tensor(\"dense_14/truediv:0\", shape=(None, 300, 3000), dtype=float32)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 300, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 300, 100)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 300, 256), ( 365568      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 300, 256), ( 365568      input_16[0][0]                   \n",
      "                                                                 lstm_7[0][1]                     \n",
      "                                                                 lstm_7[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 300, 5000)    1285000     lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 300, 5000)    1285000     lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_7 (Dot)                     (None, 300, 300)     0           dense_11[0][0]                   \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 300, 300)     90300       dot_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_8 (Dot)                     (None, 300, 256)     0           dense_12[0][0]                   \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300, 512)     0           dot_8[0][0]                      \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 300, 3000)    1539000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 300, 3000)    9003000     dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 13,933,436\n",
      "Trainable params: 13,933,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq_Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.att.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델구현 끝!\n",
    "\n",
    "### 해야할 것\n",
    "- 전처리된 데이터(embedded vector) 집어넣고 모델학습\n",
    "- hyperparameter tuning\n",
    "\n",
    "- test 데이터로 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
