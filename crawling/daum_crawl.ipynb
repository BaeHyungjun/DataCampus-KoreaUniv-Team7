{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음 뉴스 크롤링\n",
    "- 최근 2년 분야별 상위랭킹 50개 기사의 {게시날짜, 제목, 본문, 요약본, 링크} 스크래핑\n",
    "- news.db 데이터베이스 내 daumnews 테이블에 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import request\n",
    "from requests.compat import urljoin, urlparse\n",
    "from requests.exceptions import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, params={}, headers={}, method='GET', limit=3):\n",
    "    try:\n",
    "        resp = request(method, url,\n",
    "               params=params if method=='GET' else {},\n",
    "               data=params if method=='POST' else {},\n",
    "               headers=headers)\n",
    "        resp.raise_for_status()\n",
    "    except HTTPError as e:\n",
    "        if limit > 0 and e.response.status_code >= 500:\n",
    "            print(limit)\n",
    "            time.sleep(random.random()*5) # => random\n",
    "            resp = download(url, params, headers, method, limit-1)\n",
    "        else:\n",
    "            print('[{}] '.format(e.response.status_code) + url)\n",
    "            print(e.response.status_code)\n",
    "            print(e.response.reason)\n",
    "            print(e.response.headers)\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_newslink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newslink(url_base, params, headers):\n",
    "    kind = ['news', 'entertain', 'sports']\n",
    "    newslink = []\n",
    "    \n",
    "    # get dateslist (20180801~20200731 (총 730일))\n",
    "    start = datetime.strptime('20180801', '%Y%m%d') \n",
    "    end = datetime.strptime('20200731', '%Y%m%d')\n",
    "    datelst = [] \n",
    "    while start.strftime('%Y%m%d') != end.strftime('%Y%m%d'): \n",
    "        datelst.append(start.strftime('%Y%m%d')) \n",
    "        start += timedelta(days=1)\n",
    "        \n",
    "    # get links\n",
    "    for i in kind:\n",
    "        for j in datelst:\n",
    "\n",
    "            url = url_base + '/' + i\n",
    "            params['regDate']=j\n",
    "\n",
    "            resp = download(url, params=params, headers=headers)\n",
    "            if resp.status_code!=200:\n",
    "                continue\n",
    "            dom = BeautifulSoup(resp.content, 'lxml')\n",
    "\n",
    "            for _ in dom.select('.list_news2 .cont_thumb > strong > a'):\n",
    "                if _['href'] not in newslink:\n",
    "                    newslink.append(_['href'])\n",
    "    return newslink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_newstext()\n",
    "- 제목, 기사작성일, 본문, 요약 crawling\n",
    "- 기사작성일: 수정일시는 없는 기사도 많았으므로 입력일시를 사용\n",
    "- 기사 자동요약이 없는 경우에는 기사는 데이터 수집하지 않도록 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newstext(url):\n",
    "    # url 받아서 DOM 객체 만들기\n",
    "    html = requests.get(url).text\n",
    "    dom = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # summary 버튼 유무 확인하고 있으면 data 가져오기\n",
    "    if not dom.select('.btn_summary + div p'):\n",
    "        return None\n",
    "    else:\n",
    "        # 제목\n",
    "        title = dom.select_one('em + h3').text.strip()  \n",
    "\n",
    "        # 기사작성일\n",
    "        date = dom.select_one('.info_view > span + span > span').text.strip()\n",
    "        \n",
    "        # 본문\n",
    "        body_list = []\n",
    "        for _ in dom.select('.article_view > section > p'):\n",
    "            body_list.append(_.text.strip())\n",
    "        body_list.pop() # 기자 이메일 제거\n",
    "        body = '\\n'.join(body_list)\n",
    "        \n",
    "        # 요약\n",
    "        summary_list = []\n",
    "        for _ in dom.select('.btn_summary + div p'):\n",
    "            summary_list.append(_.text.strip())\n",
    "        summary = '\\n'.join(summary_list)\n",
    "\n",
    "    return title, date, body, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news.db는 data 디렉터리 안에 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('../data/news.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x29581257e30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.executescript('''\n",
    "    DROP TABLE IF EXISTS daumnews;\n",
    "    CREATE TABLE daumnews(\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        date TEXT,\n",
    "        body TEXT,\n",
    "        summary TEXT,\n",
    "        link TEXT NOT NULL\n",
    "    );\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT TO DB (뉴스링크, 내용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://news.daum.net/ranking/popular'\n",
    "params = {}\n",
    "headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'\n",
    "        }\n",
    "\n",
    "newslink = get_newslink(url_base, params, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in newslink:\n",
    "\n",
    "    title, date, body, summary = get_newstext(_)\n",
    "\n",
    "    # INSERT TO DB\n",
    "    cur.execute('INSERT INTO daumnews VALUES(NULL,?,?,?,?,?)',[title,date,body,summary, _])\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터프레임으로 바꿀 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_crawed_df(url_list):\n",
    "    col_names = ['title', 'date', 'body', 'summary', 'link']\n",
    "    rows = []\n",
    "    \n",
    "    for _ in url_list:\n",
    "        if get_newstext(_) != None:\n",
    "            rows.append(get_newstext(_) + [_])\n",
    "        \n",
    "    df = pd.DataFrame(rows, columns=col_names)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://news.v.daum.net/v/20200728064119204', 'https://news.v.daum.net/v/20200728171755863',\n",
    "           'https://news.v.daum.net/v/20200728194024562', 'https://news.v.daum.net/v/20200728214416635']\n",
    "make_crawed_df(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.718px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
