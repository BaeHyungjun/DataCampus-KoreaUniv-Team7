{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convert_token_to_id_and_padding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIk-0D0sz7uT",
        "colab_type": "text"
      },
      "source": [
        "코랩\n",
        "\n",
        "여기서는 램 12기가까지 지원하는데 얼마나 썼는지 계속 확인할 수 있어서 확인해보니\n",
        "\n",
        "이제까지 몇십시간씩 걸린건 역시 램이 딸려서 작동을 멈춰버린 상태여서 그런거였네요\n",
        "\n",
        "여기서도 20만개 한번에 돌리니까 메모리 에러 나서 5만개씩 쪼개서 4번 돌려야겠어요\n",
        "\n",
        "메모리 문제때문에 pickle로 내렸다가 올렸다가 반복하면서 했습니다\n",
        "\n",
        "요약하면 ast.literal_eval => convert token to id => padding 했어요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pjp2swRshl9",
        "colab_type": "text"
      },
      "source": [
        "token id로 바꾸는 과정에서 여기서 제공하는 어휘사전이랑 hannanum에서 제공하는 어휘 사전이랑 달라서 인식못하는 단어 => 정수 인코딩에서 0으로 변환하는 경우가 많이 보이더라고요.\n",
        "\n",
        "만약 모델링을 했는데 성능이 안좋으면\n",
        "\n",
        "1. tokenizing을 바꾸기\n",
        "\n",
        "2. token => id로 바꾸는 방법을 바꾸기\n",
        "\n",
        "등의 방법을 고려해볼 수 있을 것 같아요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cygojTloaowD",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "pip install kobert-transformers\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IECWgsLpbhPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "\n",
        "import csv\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "from kobert_transformers import get_tokenizer\n",
        "# from kobert_transformers import get_kobert_model"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezoPaP1PpaKe",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# 구글 드라이브 마운트 시키기\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mnt')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/mnt/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0, nb_path)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXFKfNeOt5Q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bef5026a-cdf6-4973-9d92-4d38a666c2a6"
      },
      "source": [
        "cd /content/mnt/My Drive/Colab Notebooks"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mnt/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KezvIo2mpCpP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "9173e83e-d39a-4eb2-8793-5090106944e3"
      },
      "source": [
        "data = pd.read_csv('news_normalized.csv')\n",
        "data.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200000 entries, 0 to 199999\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count   Dtype \n",
            "---  ------          --------------   ----- \n",
            " 0   id              200000 non-null  int64 \n",
            " 1   title           200000 non-null  object\n",
            " 2   date            200000 non-null  object\n",
            " 3   body            200000 non-null  object\n",
            " 4   summary         200000 non-null  object\n",
            " 5   link            200000 non-null  object\n",
            " 6   body_morphs     200000 non-null  object\n",
            " 7   summary_morphs  200000 non-null  object\n",
            " 8   site            200000 non-null  object\n",
            "dtypes: int64(1), object(8)\n",
            "memory usage: 13.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFQJPHuj3IIC",
        "colab_type": "text"
      },
      "source": [
        "# body_morphs에 ast.literal_eval 시켜주기 => token 파일\n",
        "\n",
        "body_morphs_list_0.pkl\n",
        "\n",
        "body_morphs_list_1.pkl\n",
        "\n",
        "body_morphs_list_2.pkl\n",
        "\n",
        "body_morphs_list_3.pkl\n",
        "\n",
        "파일 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlvkgvGBwxwc",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_0 = []\n",
        "\n",
        "for element in data.body_morphs[:50000]:\n",
        "    body_morphs_list_0.append(ast.literal_eval(element))\n",
        "      \n",
        "with open('body_morphs_list_0.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_0, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ65qk8zxL-v",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_1 = []\n",
        "\n",
        "for element in data.body_morphs[50000:100000]:\n",
        "    body_morphs_list_1.append(ast.literal_eval(element))\n",
        "\n",
        "with open('body_morphs_list_1.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_1, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcrIGPo81X2e",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_2 = []\n",
        "\n",
        "for element in data.body_morphs[100000:150000]:\n",
        "    body_morphs_list_2.append(ast.literal_eval(element))\n",
        "\n",
        "with open('body_morphs_list_2.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_2, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxpim7Sl1Yv-",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_list_3 = []\n",
        "\n",
        "for element in data.body_morphs[150000:]:\n",
        "    body_morphs_list_3.append(ast.literal_eval(element))\n",
        "\n",
        "with open('body_morphs_list_3.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_list_3, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrWhQ89y6YfG",
        "colab_type": "text"
      },
      "source": [
        "# 다시 읽어들여서 token을 token id로 변환\n",
        "\n",
        "bert의 maximum sequence length가 512이므로 token id를 512번째까지만 저장\n",
        "\n",
        "512 미만으로 token length를 가지고 있는 데이터는 뒤에서 zero padding을 해줄 예정\n",
        "\n",
        "Hannanum으로 tokenizing을 해서 kobert_tokenizer가 인식 못하는 token은 token_id가 0으로 변환되었음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj95DP9d68qX",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "tokenizer = get_tokenizer()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVrF9sa5OHV",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "with open('body_morphs_list_0.pkl', 'rb') as file:\n",
        "    body_morphs_list_0 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_0 = []\n",
        "\n",
        "for element in body_morphs_list_0:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_0.append(element_id[:512])\n",
        "\n",
        "with open('body_morphs_list_1.pkl', 'rb') as file:\n",
        "    body_morphs_list_1 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_1 = []\n",
        "\n",
        "for element in body_morphs_list_1:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_1.append(element_id[:512])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBtK2PZj9d-e",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_token_id_01 = body_morphs_token_id_0 + body_morphs_token_id_1\n",
        "\n",
        "with open('body_morphs_token_id_01.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_token_id_01, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fGrMUrG8xmy",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "with open('body_morphs_list_2.pkl', 'rb') as file:\n",
        "    body_morphs_list_2 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_2 = []\n",
        "\n",
        "for element in body_morphs_list_2:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_2.append(element_id[:512])\n",
        "\n",
        "with open('body_morphs_list_3.pkl', 'rb') as file:\n",
        "    body_morphs_list_3 = pickle.load(file)\n",
        "\n",
        "body_morphs_token_id_3 = []\n",
        "\n",
        "for element in body_morphs_list_3:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  body_morphs_token_id_3.append(element_id[:512])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2vIu61S80gH",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_morphs_token_id_23 = body_morphs_token_id_2 + body_morphs_token_id_3\n",
        "\n",
        "with open('body_morphs_token_id_23.pkl', 'wb') as file:\n",
        "  pickle.dump(body_morphs_token_id_23, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8miUVs0_WVh",
        "colab_type": "text"
      },
      "source": [
        "# token_id 불러와서 kobert_embedding 작업"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k24uXiyzAE0B",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "with open('body_morphs_token_id_01.pkl', 'rb') as file:\n",
        "  body_morphs_token_id_01 = pickle.load(file)\n",
        "\n",
        "with open('body_morphs_token_id_23.pkl', 'rb') as file:\n",
        "  body_morphs_token_id_23 = pickle.load(file)\n",
        "\n",
        "body_token_id = body_morphs_token_id_01 + body_morphs_token_id_23\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMMQbRNPCGyJ",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "pad = tokenizer.pad_token_id\n",
        "\n",
        "print(pad) # 출력값 : 1\n",
        "\n",
        "# dir(tokenizer)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYjem3XgKkl8",
        "colab_type": "text"
      },
      "source": [
        "# zero padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWDTyGv8BUKc",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# 길이가 512 미만인 애들 뒤쪽에 padding\n",
        "# 0은 tokenizer의 convert_token_to_id가 모르는 단어, 1은 패딩\n",
        "\n",
        "body_token_id_padding = []\n",
        "\n",
        "for element in body_token_id:\n",
        "  if len(element) < 512:\n",
        "    pad_element = element + [pad] * (512 - len(element))\n",
        "  else:\n",
        "    pad_element = element\n",
        "\n",
        "  body_token_id_padding.append(pad_element)\n",
        "\n",
        "with open('body_token_id_padding.pkl', 'wb') as file:\n",
        "  pickle.dump(body_token_id_padding, file)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqQhJsIXA5as",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "model = get_kobert_model()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba-Zzs7bKXdx",
        "colab_type": "text"
      },
      "source": [
        "# embedding 단계 => 모델링에서 하는걸로"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4yRP6B6BgiO",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "body_sequence_output = tf.convert_to_tensor(model(torch.LongTensor([body_token_id_padding[0]]))[0].detach())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA0MRxiNpsY2",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "start = time.time()\n",
        "for number in range(1, len(body_token_id_padding[0:20000])):\n",
        "  body_sequence_output = tf.concat([body_sequence_output,\n",
        "                                    tf.convert_to_tensor(model(torch.LongTensor([body_token_id_padding[number]]))[0].detach())], axis=0)\n",
        "  \n",
        "  if number % 2000 == 0:\n",
        "    now = time.time()\n",
        "    print('{}번째 작업, 걸린 시간 : {}분 {}초'.format(number,\n",
        "                                            round((now - start) // 60),\n",
        "                                            round((now - start) % 60)))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSNHNsck8aYa",
        "colab_type": "text"
      },
      "source": [
        "# summary도 body와 똑같이 해주기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oexN5r9EorOX",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "summary_morphs_list = []\n",
        " \n",
        "for element in data.summary_morphs:\n",
        "    summary_morphs_list.append(ast.literal_eval(element))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLP6C9ElpNVG",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "tokenizer = get_tokenizer()\n",
        "pad = tokenizer.pad_token_id\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFYjphpapxrU",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "summary_token_id = []\n",
        "\n",
        "for element in summary_morphs_list:\n",
        "  element_id = tokenizer.convert_tokens_to_ids(element)\n",
        "  summary_token_id.append(element_id[:512])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSOtm2_fqOG1",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "summary_token_id_padding = []\n",
        " \n",
        "for element in summary_token_id:\n",
        "  if len(element) < 512:\n",
        "    pad_element = element + [pad] * (512 - len(element))\n",
        "  else:\n",
        "    pad_element = element\n",
        " \n",
        "  summary_token_id_padding.append(pad_element)\n",
        " \n",
        "with open('summary_token_id_padding.pkl', 'wb') as file:\n",
        "  pickle.dump(summary_token_id_padding, file) \n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T597ty2rOi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
